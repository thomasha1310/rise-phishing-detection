{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2b7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import svm\n",
    "import lime\n",
    "import lime.lime_text\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d512b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uname_result(system='Darwin', node='crc-dot1x-nat-10-239-237-84.bu.edu', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 18:40:14 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T8103', machine='arm64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.uname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c80117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(text: str) -> str:\n",
    "    text = re.sub(r'<[^>]+>', '', text) # remove HTML tags\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    # TODO: add a count of URLs to email data\n",
    "    text = re.sub(r'\\d+', '', text) # remove numerical text\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    text = text.lower().strip() # lowercase\n",
    "    return text\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df['clean_email'] = df['body'].astype(str).apply(clean_email)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09a3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/emails_augmented.csv')  # Update path if necessary\n",
    "assert 'body' in df.columns and 'label' in df.columns, \"Missing required columns.\"\n",
    "df = preprocess(df)\n",
    "X = df['clean_email']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e035ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['clean_email'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d148b336",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      2\u001b[39m results = []\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e172ab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Different Models\u001b[39;00m\n\u001b[32m      2\u001b[39m model = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model.fit(X_train, y_train)\n\u001b[32m      4\u001b[39m y_pred = model.predict(X_test)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClassification Report:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Different Models\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478b1f48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Logistic Regression with Grid Search and K-fold cross-validation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = LogisticRegression()\n\u001b[32m      4\u001b[39m param_grid = [\n\u001b[32m      5\u001b[39m     {\n\u001b[32m      6\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpenalty\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33ml1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     }\n\u001b[32m     17\u001b[39m ]\n\u001b[32m     18\u001b[39m kfold = KFold(n_splits=\u001b[32m10\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "#Logistic Regression with Grid Search and K-fold cross-validation\n",
    "model = LogisticRegression()\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['liblinear'],  # Efficient for sparse text features\n",
    "        'max_iter': [500]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['saga'],  # Faster on large data\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "]\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y) \n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'LogisticRegression()',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0787723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     39527\n",
      "           1       0.99      0.99      0.99     42611\n",
      "\n",
      "    accuracy                           0.99     82138\n",
      "   macro avg       0.99      0.99      0.99     82138\n",
      "weighted avg       0.99      0.99      0.99     82138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3b6402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\\ngnb = GaussianNB()\\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\\nprint(\"Classification Report:\")\\nprint(classification_report(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43424bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best parameters found:  {'alpha': 0.01, 'fit_prior': True}\n",
      "Best cross-validation score:  0.9334652259929996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93      7815\n",
      "           1       0.98      0.89      0.93      8613\n",
      "\n",
      "    accuracy                           0.93     16428\n",
      "   macro avg       0.93      0.93      0.93     16428\n",
      "weighted avg       0.94      0.93      0.93     16428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive Bayes with Grid Search and K-fold cross-validation\n",
    "model = MultinomialNB()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0],  \n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'MultinomialNB',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb91126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93     39527\n",
      "           1       0.98      0.89      0.93     42611\n",
      "\n",
      "    accuracy                           0.93     82138\n",
      "   macro avg       0.94      0.94      0.93     82138\n",
      "weighted avg       0.94      0.93      0.93     82138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0f76ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn import svm\\nX_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\\nclf = svm.SVC()\\ny_pred = clf.fit(X_train, y_train).predict(X_test)\\nprint(\"Classification Report:\")\\nprint(classification_report(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn import svm\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
    "clf = svm.SVC()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b891034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     16\u001b[39m kfold = KFold(n_splits=\u001b[32m10\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     18\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     19\u001b[39m     estimator=model,\n\u001b[32m     20\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m grid_search.fit(X, y) \n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters found: \u001b[39m\u001b[33m\"\u001b[39m, grid_search.best_params_)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest cross-validation score: \u001b[39m\u001b[33m\"\u001b[39m, grid_search.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28mself\u001b[39m._run_search(evaluate_candidates)\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m.param_grid))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = parallel(\n\u001b[32m    971\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    972\u001b[39m         clone(base_estimator),\n\u001b[32m    973\u001b[39m         X,\n\u001b[32m    974\u001b[39m         y,\n\u001b[32m    975\u001b[39m         train=train,\n\u001b[32m    976\u001b[39m         test=test,\n\u001b[32m    977\u001b[39m         parameters=parameters,\n\u001b[32m    978\u001b[39m         split_progress=(split_idx, n_splits),\n\u001b[32m    979\u001b[39m         candidate_progress=(cand_idx, n_candidates),\n\u001b[32m    980\u001b[39m         **fit_and_score_kwargs,\n\u001b[32m    981\u001b[39m     )\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[32m    983\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[32m    984\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(cv.split(X, y, **routed_params.splitter.split)),\n\u001b[32m    985\u001b[39m     )\n\u001b[32m    986\u001b[39m )\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Support Vector Classification with Grid Search and K-fold cross-validation\n",
    "model = svm.SVC()\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': [1e-3, 1e-4]\n",
    "    }\n",
    "]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y) \n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'SVC',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b4ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      7815\n",
      "           1       0.93      0.97      0.95      8613\n",
      "\n",
      "    accuracy                           0.95     16428\n",
      "   macro avg       0.95      0.94      0.95     16428\n",
      "weighted avg       0.95      0.95      0.95     16428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#default random forest \n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d35605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 252 candidates, totalling 2520 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     11\u001b[39m kfold = KFold(n_splits=\u001b[32m10\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     12\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     13\u001b[39m     estimator=model,\n\u001b[32m     14\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m grid_search.fit(X, y) \n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters found: \u001b[39m\u001b[33m\"\u001b[39m, grid_search.best_params_)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest cross-validation score: \u001b[39m\u001b[33m\"\u001b[39m, grid_search.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28mself\u001b[39m._run_search(evaluate_candidates)\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m.param_grid))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = parallel(\n\u001b[32m    971\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    972\u001b[39m         clone(base_estimator),\n\u001b[32m    973\u001b[39m         X,\n\u001b[32m    974\u001b[39m         y,\n\u001b[32m    975\u001b[39m         train=train,\n\u001b[32m    976\u001b[39m         test=test,\n\u001b[32m    977\u001b[39m         parameters=parameters,\n\u001b[32m    978\u001b[39m         split_progress=(split_idx, n_splits),\n\u001b[32m    979\u001b[39m         candidate_progress=(cand_idx, n_candidates),\n\u001b[32m    980\u001b[39m         **fit_and_score_kwargs,\n\u001b[32m    981\u001b[39m     )\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[32m    983\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[32m    984\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(cv.split(X, y, **routed_params.splitter.split)),\n\u001b[32m    985\u001b[39m     )\n\u001b[32m    986\u001b[39m )\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Random forest with Grid Search and K-fold cross-validation\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],              \n",
    "    'max_depth': [10, 20, None],             \n",
    "    'max_features': ['sqrt', 'log2'],        \n",
    "    'min_samples_leaf': [1, 2]               \n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y) \n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Random Forest Classifier',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coefs = model.coef_[0]\n",
    "top_phishing_idx = np.argsort(coefs)[-10:]\n",
    "top_legit_idx = np.argsort(coefs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5076a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fae64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Melt the DataFrame for seaborn\n",
    "metrics_df = results.melt(id_vars='Model', \n",
    "                             value_vars=['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1'], \n",
    "                             var_name='Metric', \n",
    "                             value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=metrics_df, x='Model', y='Score', hue='Metric')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b6236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             subject  \\\n",
      "0                          Never agree to be a loser   \n",
      "1                             Befriend Jenna Jameson   \n",
      "2                               CNN.com Daily Top 10   \n",
      "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
      "4                         SpecialPricesPharmMoreinfo   \n",
      "\n",
      "                                                body  label  \n",
      "0  Buck up, your troubles caused by small dimensi...      1  \n",
      "1  \\nUpgrade your sex and pleasures with these te...      1  \n",
      "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1  \n",
      "3  Would anyone object to removing .so from this ...      0  \n",
      "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 806/8214 [26:57<4:03:59,  1.98s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    " \n",
    "\n",
    "df = pd.read_csv('data/emails.csv')\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.iloc[idx]['body'])\n",
    "        label = int(self.df.iloc[idx]['label'])\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encodings['input_ids'].squeeze()\n",
    "        attention_mask = encodings['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "\n",
    "train_ds = EmailDataset(train_df, tokenizer)\n",
    "val_ds = EmailDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "\n",
    "num_labels = df['label'].nunique()\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Validation Accuracy: {acc:.2%}\")\n",
    "\n",
    "model.save_pretrained('./phishing-bert-model')\n",
    "tokenizer.save_pretrained('./phishing-bert-model')\n",
    "print(\"Model saved to ./phishing-bert-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./phishing-bert-model')\n",
    "model = BertForSequenceClassification.from_pretrained('./phishing-bert-model')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "email_text = \"\"\"\n",
    "California State Department of Vehicles(DMV) Final Notice: Enforcement Penalties Begin on July 25.\n",
    "Our records show that as of today, you still have an outstanding traffic ticket. In accordance with  California State Administrative Code 15C-16.003, If you do not complete payment by July 24, 2025, we will take the following actions:\n",
    "\n",
    "1. Report to the DMV violation database\n",
    "2. Suspend your vehicle registration starting July 25\n",
    "3. Suspend driving privileges for 30 days\n",
    "4. Transfer to a toll booth and charge a 35% service fee\n",
    "5. You may be prosecuted and your credit score will be affected\n",
    "Pay Now:\n",
    "\n",
    "https://cadmv.com-tieq.run/pay\n",
    "\n",
    "Please pay immediately before enforcement to avoid license suspension and further legal disputes.\n",
    "(Reply Y and re-open this message to click the link, or copy it to your browser.)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def predict_proba(texts):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).numpy()\n",
    "    return probs\n",
    "\n",
    "\n",
    "probs = predict_proba([email_text])[0]\n",
    "pred_label = probs.argmax()\n",
    "print(f\"Prediction: {pred_label} | Prob: {probs[pred_label]:.2%}\")\n",
    "\n",
    "\n",
    "explainer = shap.Explainer(\n",
    "    predict_proba,\n",
    "    masker=shap.maskers.Text(tokenizer)\n",
    ")\n",
    "\n",
    "shap_values = explainer([email_text])\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top indicative words for phishing:\n",
      "remove: 3.8784\n",
      "our: 3.9179\n",
      "investment: 3.9707\n",
      "men: 4.1906\n",
      "money: 4.2702\n",
      "http: 4.6352\n",
      "you: 5.0106\n",
      "love: 5.3526\n",
      "josemonkeyorg: 6.0801\n",
      "your: 6.5893\n",
      "\n",
      "Top indicative words for legitimate:\n",
      "enron: -11.0770\n",
      "wrote: -10.2384\n",
      "thanks: -8.1074\n",
      "url: -6.6347\n",
      "vince: -6.1347\n",
      "pm: -6.1271\n",
      "louise: -5.7562\n",
      "date: -5.6297\n",
      "feb: -4.7627\n",
      "university: -4.7434\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop indicative words for phishing:\")\n",
    "for word, coef in zip(feature_names[top_phishing_idx], coefs[top_phishing_idx]):\n",
    "    print(f\"{word}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nTop indicative words for legitimate:\")\n",
    "for word, coef in zip(feature_names[top_legit_idx], coefs[top_legit_idx]):\n",
    "    print(f\"{word}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(vectorizer, model)\n",
    "explainer = lime.lime_text.LimeTextExplainer(class_names=['Legitimate', 'Phishing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explaining instance: cnn alerts my custom alert\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "alert name my custom alert\n",
      "girl surives  storey fall\n",
      "\n",
      "fri  aug   \n",
      "\n",
      "full story\n",
      "\n",
      "\n",
      "\n",
      "you have agreed to receive this email from cnncom as a result of your cnncom preference settings\n",
      "to manage your settings click here\n",
      "to alter your alert criteria or frequency or to unsubscribe from receiving custom email alerts click here\n",
      "\n",
      "\n",
      "cable news network one cnn center atlanta georgia \n",
      "©  cable news network\n",
      "a time warner company\n",
      "all rights reserved\n",
      "view our privacy policy and terms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, len(df) - 1)\n",
    "print(\"\\nExplaining instance:\", df['clean_email'].iloc[idx])\n",
    "exp = explainer.explain_instance(df['clean_email'].iloc[idx], pipeline.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = './generated'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "exp.save_to_file(os.path.join(output_dir, 'lime_explanation.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, './generated/phishing_model.pkl')\n",
    "joblib.dump(vectorizer, './generated/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"\\nModel and vectorizer saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
