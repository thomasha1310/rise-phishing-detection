{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/analysis/emails_augmented.csv')\n",
    "assert 'body_no_stopwords' in df.columns and 'label' in df.columns, \"Missing required columns.\"\n",
    "X = df['body_no_stopwords']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1 /projectnb/rise-phishing/andrewhl/.conda/envs/phishfence/lib/python3.13/site-packages/nltk/__init__.py True\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__, nltk.__file__, hasattr(nltk, \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import torch   \n",
    "torch.set_num_threads(16)\n",
    "print(torch.get_num_threads())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /usr4/spclpgm/andrewhl/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding documents:   0%|          | 22/65644 [00:00<24:45, 44.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Embedding documents: 100%|██████████| 65644/65644 [30:47<00:00, 35.53it/s]   \n",
      "Embedding documents: 100%|██████████| 16411/16411 [07:35<00:00, 36.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "nltk.download('punkt',    quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class SBERTChunkAverageTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Split documents into sentence‐chunks, enforce a 512‐token limit (incl. special tokens),\n",
    "    embed them with SBERT, and average chunk embeddings per document.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 chunk_size: int = 5,\n",
    "                 batch_size: int = 64,\n",
    "                 show_progress_bar: bool = True,\n",
    "                 device: str = None):\n",
    "\n",
    "        self.model_name        = model_name\n",
    "        self.chunk_size        = chunk_size\n",
    "        self.batch_size        = batch_size\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "        self.device            = device\n",
    "\n",
    "        self.model             = None\n",
    "        self.tokenizer         = None\n",
    "        self._body_max_length  = None\n",
    "\n",
    "    def _ensure_model_and_tokenizer(self):\n",
    "        if self.model is None:\n",
    "            self.model = SentenceTransformer(self.model_name,\n",
    "                                             device=self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            special_ids = self.tokenizer.encode(\"\", add_special_tokens=True)\n",
    "            n_special   = len(special_ids)\n",
    "            self._body_max_length = self.tokenizer.model_max_length - n_special\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self._ensure_model_and_tokenizer()\n",
    "        return self\n",
    "\n",
    "    def _split_into_token_safe_subchunks(self, text_chunk: str):\n",
    "        \"\"\"\n",
    "        Given a text chunk, split it into pieces so that after adding special tokens\n",
    "        each piece is <= tokenizer.model_max_length.\n",
    "        \"\"\"\n",
    "        max_body = self._body_max_length\n",
    "        sents = [s for s in sent_tokenize(text_chunk) if s.strip()]\n",
    "        if not sents:\n",
    "            return []\n",
    "\n",
    "        out_subchunks = []\n",
    "        curr_sents   = []\n",
    "        curr_tokens  = 0\n",
    "\n",
    "        for sent in sents:\n",
    "            tok_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n",
    "            L       = len(tok_ids)\n",
    "\n",
    "            if L > max_body:\n",
    "\n",
    "                if curr_sents:\n",
    "                    out_subchunks.append(\" \".join(curr_sents))\n",
    "                    curr_sents  = []\n",
    "                    curr_tokens = 0\n",
    "\n",
    "                for i in range(0, L, max_body):\n",
    "                    piece_ids  = tok_ids[i : i + max_body]\n",
    "                    piece_text = self.tokenizer.decode(piece_ids,\n",
    "                                                       clean_up_tokenization_spaces=True)\n",
    "                    out_subchunks.append(piece_text)\n",
    "            else:\n",
    "                if curr_tokens + L <= max_body:\n",
    "                    curr_sents.append(sent)\n",
    "                    curr_tokens += L\n",
    "                else:\n",
    "\n",
    "                    out_subchunks.append(\" \".join(curr_sents))\n",
    "                    curr_sents  = [sent]\n",
    "                    curr_tokens = L\n",
    "\n",
    "\n",
    "        if curr_sents:\n",
    "            out_subchunks.append(\" \".join(curr_sents))\n",
    "\n",
    "        return out_subchunks\n",
    "\n",
    "    def _chunk_and_average(self, doc: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Splits a single document into safe sub‐chunks, encodes, and averages.\n",
    "        \"\"\"\n",
    "        self._ensure_model_and_tokenizer()\n",
    "\n",
    "\n",
    "        if not isinstance(doc, str) or not doc.strip():\n",
    "            return np.zeros(self.model.get_sentence_embedding_dimension(),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "\n",
    "        sentences = [s for s in sent_tokenize(doc) if s.strip()]\n",
    "        if not sentences:\n",
    "            return np.zeros(self.model.get_sentence_embedding_dimension(),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "        initial_chunks = []\n",
    "        for i in range(0, len(sentences), self.chunk_size):\n",
    "            block = \" \".join(sentences[i : i + self.chunk_size]).strip()\n",
    "            if block:\n",
    "                initial_chunks.append(block)\n",
    "\n",
    "\n",
    "        safe_chunks = []\n",
    "        for chunk in initial_chunks:\n",
    "            safe_chunks.extend(self._split_into_token_safe_subchunks(chunk))\n",
    "\n",
    "        if not safe_chunks:\n",
    "            return np.zeros(self.model.get_sentence_embedding_dimension(),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "        embs = self.model.encode(\n",
    "            safe_chunks,\n",
    "            batch_size=self.batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        return embs.mean(axis=0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_model_and_tokenizer()\n",
    "\n",
    "        docs_iter = tqdm(X, total=len(X), desc=\"Embedding documents\")\n",
    "        all_embs = [\n",
    "            self._chunk_and_average(doc)\n",
    "            for doc in docs_iter\n",
    "        ]\n",
    "        return np.vstack(all_embs)\n",
    "\n",
    "\n",
    "sbert_chunker = SBERTChunkAverageTransformer(batch_size=64)\n",
    "\n",
    "X_train_emb = sbert_chunker.fit_transform(X_train)\n",
    "X_test_emb  = sbert_chunker.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from joblib import dump \n",
    "output_dir = '../../output/embeddings'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dump(X_train_emb, os.path.join(output_dir, 'X_train_emb.joblib'))\n",
    "dump(X_test_emb, os.path.join(output_dir, 'X_test_emb.joblib'))\n",
    "dump(y_train, os.path.join(output_dir, 'y_train.joblib'))\n",
    "dump(y_test, os.path.join(output_dir, 'y_test.joblib'))\n",
    "\n",
    "print(\"Embeddings saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
