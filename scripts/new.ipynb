{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2b7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d512b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uname_result(system='Linux', node='scc-505', release='4.18.0-553.54.1.el8_10.x86_64', version='#1 SMP Tue May 27 22:49:52 EDT 2025', machine='x86_64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.uname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c80117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(text: str) -> str:\n",
    "    text = re.sub(r'<[^>]+>', '', text) # remove HTML tags\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    # TODO: add a count of URLs to email data\n",
    "    text = re.sub(r'\\d+', '', text) # remove numerical text\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    text = text.lower().strip() # lowercase\n",
    "    return text\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df['clean_email'] = df['body'].astype(str).apply(clean_email)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09a3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/analysis/emails_augmented.csv')  # Update path if necessary\n",
    "assert 'body' in df.columns and 'label' in df.columns, \"Missing required columns.\"\n",
    "df = preprocess(df)\n",
    "X = df['clean_email']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e035ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['clean_email'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d148b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e172ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different Models\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with Grid Search and K-fold cross-validation\n",
    "model = LogisticRegression()\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['liblinear'],  # Efficient for sparse text features\n",
    "        'max_iter': [500]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['saga'],  # Faster on large data\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "]\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y) \n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'LogisticRegression()',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43424bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes with Grid Search and K-fold cross-validation\n",
    "model = MultinomialNB()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0],  \n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'MultinomialNB',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb91126",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn import svm\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
    "clf = svm.SVC()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b891034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Classification with Grid Search and K-fold cross-validation\n",
    "model = svm.SVC()\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': [1e-3, 1e-4]\n",
    "    }\n",
    "]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y) \n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'SVC',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#default random forest \n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest with Grid Search and K-fold cross-validation\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],              \n",
    "    'max_depth': [10, 20, None],             \n",
    "    'max_features': ['sqrt', 'log2'],        \n",
    "    'min_samples_leaf': [1, 2]               \n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y) \n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Random Forest Classifier',\n",
    "    'Best Params': grid_search.best_params_,\n",
    "    'CV Accuracy': grid_search.best_score_,\n",
    "    'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'Test F1': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coefs = model.coef_[0]\n",
    "top_phishing_idx = np.argsort(coefs)[-10:]\n",
    "top_legit_idx = np.argsort(coefs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5076a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fae64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Melt the DataFrame for seaborn\n",
    "metrics_df = results.melt(id_vars='Model', \n",
    "                             value_vars=['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1'], \n",
    "                             var_name='Metric', \n",
    "                             value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=metrics_df, x='Model', y='Score', hue='Metric')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89b6236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                            subject  \\\n",
      "0           0                          Never agree to be a loser   \n",
      "1           1                             Befriend Jenna Jameson   \n",
      "2           2                               CNN.com Daily Top 10   \n",
      "3           3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
      "4           4                         SpecialPricesPharmMoreinfo   \n",
      "\n",
      "                                                body  label  num_urls  \\\n",
      "0  Buck up, your troubles caused by small dimensi...      1         1   \n",
      "1  \\nUpgrade your sex and pleasures with these te...      1         1   \n",
      "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1        29   \n",
      "3  Would anyone object to removing .so from this ...      0       939   \n",
      "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1         1   \n",
      "\n",
      "   num_words  num_chars_foreign  num_chars_special  num_urgent_words  \\\n",
      "0         46                  0                 13                 0   \n",
      "1          9                  0                  5                 0   \n",
      "2        302                  0                722                 1   \n",
      "3       2660                  0               5769                 1   \n",
      "4          2                  0                 13                 0   \n",
      "\n",
      "   num_stopwords                                  body_no_stopwords  \n",
      "0             24  buck troubles caused small dimension soon beco...  \n",
      "1              4              upgrade sex pleasures techniques http  \n",
      "2             64  daily top top videos stories aug pm edt top vi...  \n",
      "3            142  would anyone object removing list tld basicall...  \n",
      "4              0            welcomefastshippingcustomersupport http  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8206/8206 [05:17<00:00, 25.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0669\n",
      "Validation Accuracy: 98.92%\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8206/8206 [05:17<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0143\n",
      "Validation Accuracy: 99.21%\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8206/8206 [05:17<00:00, 25.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0029\n",
      "Validation Accuracy: 99.21%\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8206/8206 [05:17<00:00, 25.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0009\n",
      "Validation Accuracy: 99.21%\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8206/8206 [05:17<00:00, 25.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0009\n",
      "Validation Accuracy: 99.21%\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8206/8206 [05:17<00:00, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0009\n",
      "Validation Accuracy: 99.21%\n",
      "Model saved to ./phishing-bert-model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    " \n",
    "\n",
    "df = pd.read_csv('../data/analysis/emails_augmented.csv')\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.iloc[idx]['body_no_stopwords'])\n",
    "        label = int(self.df.iloc[idx]['label'])\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encodings['input_ids'].squeeze()\n",
    "        attention_mask = encodings['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "\n",
    "train_ds = EmailDataset(train_df, tokenizer)\n",
    "val_ds = EmailDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "\n",
    "num_labels = df['label'].nunique()\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(6):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Validation Accuracy: {acc:.2%}\")\n",
    "\n",
    "model.save_pretrained('./phishing-bert-model')\n",
    "tokenizer.save_pretrained('./phishing-bert-model')\n",
    "print(\"Model saved to ./phishing-bert-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7186d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved detailed confusion matrix with borders to BERT_confusion_matrix.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    126\u001b[39m y_true_val, y_pred_val = [], []\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mEmailDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     35\u001b[39m text = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.df.iloc[idx][\u001b[33m'\u001b[39m\u001b[33mbody_no_stopwords\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     36\u001b[39m label = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.df.iloc[idx][\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     45\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m: encodings[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].squeeze(),\n\u001b[32m     46\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m: encodings[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].squeeze(),\n\u001b[32m     47\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: torch.tensor(label)\n\u001b[32m     48\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2855\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2854\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2965\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2943\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2944\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2945\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2962\u001b[39m         **kwargs,\n\u001b[32m   2963\u001b[39m     )\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2968\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3040\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3011\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3012\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3013\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3028\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3029\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3031\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3032\u001b[39m     padding=padding,\n\u001b[32m   3033\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3037\u001b[39m     **kwargs,\n\u001b[32m   3038\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3040\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/tokenization_utils.py:800\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_for_model(\n\u001b[32m    804\u001b[39m     first_ids,\n\u001b[32m    805\u001b[39m     pair_ids=second_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     verbose=verbose,\n\u001b[32m    821\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/tokenization_utils.py:767\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/models/bert/tokenization_bert.py:161\u001b[39m, in \u001b[36mBertTokenizer._tokenize\u001b[39m\u001b[34m(self, text, split_special_tokens)\u001b[39m\n\u001b[32m    159\u001b[39m split_tokens = []\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_basic_tokenize:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[32m    165\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.basic_tokenizer.never_split:\n\u001b[32m    166\u001b[39m             split_tokens.append(token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/models/bert/tokenization_bert.py:329\u001b[39m, in \u001b[36mBasicTokenizer.tokenize\u001b[39m\u001b[34m(self, text, never_split)\u001b[39m\n\u001b[32m    327\u001b[39m     token = token.lower()\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strip_accents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m         token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_strip_accents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strip_accents:\n\u001b[32m    331\u001b[39m     token = \u001b[38;5;28mself\u001b[39m._run_strip_accents(token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/transformers/models/bert/tokenization_bert.py:345\u001b[39m, in \u001b[36mBasicTokenizer._run_strip_accents\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cat == \u001b[33m\"\u001b[39m\u001b[33mMn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    344\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     output.append(char)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(output)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Load Model & Tokenizer ----\n",
    "MODEL_PATH = \"./phishing-bert-model\"\n",
    "OUTPUT_CM = \"BERT_confusion_matrix.png\"\n",
    "OUTPUT_LC = \"BERT_learning_curve.png\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ---- Reload Dataset ----\n",
    "df = pd.read_csv('../data/analysis/emails_augmented.csv')\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.iloc[idx]['body_no_stopwords'])\n",
    "        label = int(self.df.iloc[idx]['label'])\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "val_ds = EmailDataset(val_df, tokenizer)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ---- Confusion Matrix with Borders and Custom Labels ----\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_sum = np.sum(cm)\n",
    "cm_perc = cm / cm_sum * 100\n",
    "\n",
    "labels = np.array([[\"True Neg\", \"False Pos\"], [\"False Neg\", \"True Pos\"]])\n",
    "annot = np.empty_like(cm).astype(str)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        c = cm[i, j]\n",
    "        p = cm_perc[i, j]\n",
    "        label = labels[i, j]\n",
    "        annot[i, j] = f\"{label}\\n{c}\\n{p:.1f}%\"\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=annot,\n",
    "    fmt=\"\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    "    linewidths=1, linecolor=\"black\",  # <-- keeps box borders\n",
    "    square=True\n",
    ")\n",
    "plt.title(\"BERT - Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.savefig(OUTPUT_CM, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved detailed confusion matrix with borders to {OUTPUT_CM}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---- Learning Curve ----\n",
    "train_sizes = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "train_scores, val_scores = [], []\n",
    "\n",
    "for frac in train_sizes:\n",
    "    # use full set if frac == 1.0, else sample\n",
    "    frac_train_df = train_df.sample(frac=frac, random_state=42) if frac < 1.0 else train_df.copy()\n",
    "    frac_train_ds = EmailDataset(frac_train_df, tokenizer)\n",
    "    frac_train_loader = DataLoader(frac_train_ds, batch_size=16)\n",
    "\n",
    "    # Training Accuracy\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in frac_train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            y_true_train.extend(labels.cpu().numpy())\n",
    "            y_pred_train.extend(preds.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "\n",
    "    # Validation Accuracy\n",
    "    y_true_val, y_pred_val = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            y_true_val.extend(labels.cpu().numpy())\n",
    "            y_pred_val.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "\n",
    "    train_scores.append(train_acc)\n",
    "    val_scores.append(val_acc)\n",
    "\n",
    "# Plot Learning Curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.array(train_sizes) * len(train_df), train_scores, 'o-', color=\"blue\", label=\"Training Score\")\n",
    "plt.plot(np.array(train_sizes) * len(train_df), val_scores, 'o-', color=\"green\", label=\"Validation Score\")\n",
    "plt.title(\"Learning Curve - BERT\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(OUTPUT_LC, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved learning curve to {OUTPUT_LC}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f077f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/rise-phishing/connorhl/.conda/envs/phishing_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./phishing-bert-model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./phishing-bert-model')\n",
    "model.eval()\n",
    "\n",
    "def classify_email(email_text):\n",
    "    inputs = tokenizer(email_text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1).squeeze().numpy()\n",
    "    label = int(probs.argmax())\n",
    "    return label, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b34c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def gpt_explanation(email_text):\n",
    "    prompt = f\"\"\"The following email was classified as a phishing attempt. Provide a brief (1-2 sentence) explanation of why the email is likely to be spam and a 1 sentence recommendation for steps to take.\n",
    "\"{email_text}\"\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a cybersecurity analyst trained to explain phishing emails.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dff79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explanation:\", gpt_explanation(email_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "import shap\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./phishing-bert-model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./phishing-bert-model')\n",
    "\n",
    "# Create a transformers pipeline (SHAP expects this)\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# Email example\n",
    "email_text = \"\"\"\n",
    "Can anyone help with the following:\n",
    "\n",
    "I get:\n",
    "\n",
    " > mcmcsamp(txt.lmer3f)\n",
    "Markov Chain Monte Carlo (MCMC) output:\n",
    "Start = 1\n",
    "End = 1\n",
    "Thinning interval = 1\n",
    "....\n",
    "\n",
    "But:\n",
    "\n",
    " > mcsamp(txt.lmer3f)\n",
    "Error in as.bugs.array(sims, program = \"\"lmer\"\", n.iter = n.iter,  \n",
    "n.burnin = n.burnin,  :\n",
    "\terror in parameter sigma.s in parameters.to.save\n",
    "\n",
    "\n",
    "I had attached the arm package:\n",
    " > library(arm)\n",
    "Loading required package: R2WinBUGS\n",
    "\n",
    "arm (Version 1.0-15, built: 2007-3-17)\n",
    "Working directory is /Users/mk/Documents/projects/Mike/prediss/Exp-Text\n",
    "Loading required package: foreign\n",
    "\n",
    " > sessionInfo()\n",
    "R version 2.4.1 (2006-12-18)\n",
    "i386-apple-darwin8.8.1\n",
    "\n",
    "locale:\n",
    "C\n",
    "\n",
    "attached base packages:\n",
    "[1] \"\"tools\"\"     \"\"grid\"\"      \"\"datasets\"\"  \"\"stats\"\"     \"\"graphics\"\"   \n",
    "\"\"grDevices\"\" \"\"utils\"\"     \"\"methods\"\"\n",
    "[9] \"\"base\"\"\n",
    "\n",
    "other attached packages:\n",
    "      foreign          arm    R2WinBUGS         coda         \n",
    "Hmisc      gmodels      effects         lme4\n",
    "     \"\"0.8-18\"\"     \"\"1.0-15\"\"      \"\"2.0-4\"\"     \"\"0.10-7\"\"      \"\"3.2-1\"\"      \n",
    "\"\"2.13.2\"\"      \"\"1.0-9\"\"  \"\"0.9975-13\"\"\n",
    "       Matrix          car       weaver    codetools        \n",
    "digest       xtable latticeExtra      lattice\n",
    "\"\"0.9975-11\"\"      \"\"1.2-1\"\"      \"\"1.0.1\"\"      \"\"0.0-3\"\"      \"\"0.2.3\"\"       \n",
    "\"\"1.4-3\"\"      \"\"0.1-4\"\"    \"\"0.14-17\"\"\n",
    "     gridBase         MASS          JGR       iplots        \n",
    "JavaGD        rJava\n",
    "      \"\"0.4-3\"\"     \"\"7.2-33\"\"     \"\"1.4-15\"\"      \"\"1.0-5\"\"      \"\"0.3-6\"\"      \n",
    "\"\"0.4-14\"\"\n",
    "\n",
    "Thanks,\n",
    "MK\n",
    "\"\"\"\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.Explainer(pipe, shap.maskers.Text(tokenizer))\n",
    "shap_values = explainer([email_text])  # must be list[str]\n",
    "\n",
    "# Plot\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop indicative words for phishing:\")\n",
    "for word, coef in zip(feature_names[top_phishing_idx], coefs[top_phishing_idx]):\n",
    "    print(f\"{word}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nTop indicative words for legitimate:\")\n",
    "for word, coef in zip(feature_names[top_legit_idx], coefs[top_legit_idx]):\n",
    "    print(f\"{word}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(vectorizer, model)\n",
    "explainer = lime.lime_text.LimeTextExplainer(class_names=['Legitimate', 'Phishing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.randint(0, len(df) - 1)\n",
    "print(\"\\nExplaining instance:\", df['clean_email'].iloc[idx])\n",
    "exp = explainer.explain_instance(df['clean_email'].iloc[idx], pipeline.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = './generated'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "exp.save_to_file(os.path.join(output_dir, 'lime_explanation.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, './generated/phishing_model.pkl')\n",
    "joblib.dump(vectorizer, './generated/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"\\nModel and vectorizer saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishfence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
